{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44aaadc1-49d8-4df8-bfc3-5c817cd07db7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install omegaconf\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdfbbcfc-49e3-4f3b-9de4-7bce801aa9c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## LOG DE ERRORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80695dea-a685-4a45-9dd5-e8854afc7c7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import logging, os, shutil\n",
    "from pyspark.sql import functions as F\n",
    "import os, re\n",
    "from datetime import datetime\n",
    "\n",
    "FINAL_LOG_DIR  = \"/Volumes/workspace/global_mobility/log\"\n",
    "FINAL_LOG_PATH = os.path.join(FINAL_LOG_DIR, f\"etl_run_{datetime.now():%Y%m%d_%H%M%S}.log\")\n",
    "\n",
    "os.makedirs(FINAL_LOG_DIR, exist_ok=True)\n",
    "\n",
    "logger = logging.getLogger(\"etl_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False  # evita duplicado en el root\n",
    "\n",
    "for h in list(logger.handlers):\n",
    "    logger.removeHandler(h)\n",
    "    try:\n",
    "        h.flush()\n",
    "        h.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "fmt = logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "file_handler = logging.FileHandler(FINAL_LOG_PATH, mode=\"a\", encoding=\"utf-8\")\n",
    "file_handler.setFormatter(fmt)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(fmt)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "def log_info(msg: str):\n",
    "    logger.info(msg)\n",
    "\n",
    "def log_error(msg: str):\n",
    "    logger.error(f\" {msg}\")\n",
    "\n",
    "def cerrar_log():\n",
    "    for h in list(logger.handlers):\n",
    "        try:\n",
    "            h.flush()\n",
    "            h.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "        logger.removeHandler(h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ead5e187-1f40-4954-8a26-a211b11e2a6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## VALIDACION DEL ARCHIVO CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55fa8697-00e0-4f6f-9e87-ce6594444019",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "REPO_ROOT = os.path.dirname(os.getcwd())  \n",
    "CFG_PATH = os.path.join(REPO_ROOT, \"config\", \"config.yaml\")\n",
    "\n",
    "try:\n",
    "    # --- Verificar existencia\n",
    "    if not os.path.exists(CFG_PATH):\n",
    "        log_error(f\"Archivo de configuración no encontrado en: {CFG_PATH}\")\n",
    "        raise FileNotFoundError(f\"No existe el archivo {CFG_PATH}\")\n",
    "\n",
    "    config = OmegaConf.load(CFG_PATH)\n",
    "    log_info(\"config.yaml cargado correctamente.\")\n",
    "\n",
    "    # --- Estructura básica\n",
    "    required_keys = [\"paths\", \"params\", \"delivery_types\", \"unit_factors\"]\n",
    "    for key in required_keys:\n",
    "        if key not in config:\n",
    "            log_error(f\"Falta la sección '{key}' en config.yaml\")\n",
    "            raise ValueError(f\"Falta la sección '{key}' en config.yaml\")\n",
    "\n",
    "    # --- Validar params\n",
    "    for i, param in enumerate(config.params):\n",
    "        for field in [\"country\", \"start_date\", \"end_date\", \"proccess\"]:\n",
    "            if field not in param:\n",
    "                log_error(f\"Falta '{field}' en bloque params[{i}] del config.yaml\")\n",
    "                raise ValueError(f\"Falta '{field}' en bloque params[{i}]\")\n",
    "\n",
    "        if param.proccess not in [\"YES\", \"NO\"]:\n",
    "            log_error(f\"Valor inválido en 'proccess' ({param.proccess}) — debe ser YES o NO\")\n",
    "            raise ValueError(f\"Valor inválido en 'proccess' ({param.proccess})\")\n",
    "\n",
    "    log_info(f\"params OK ({len(config.params)} bloques validados).\")\n",
    "\n",
    "    # --- Validar delivery_types\n",
    "    if \"routine\" not in config.delivery_types or \"bonus\" not in config.delivery_types:\n",
    "        log_error(\"Faltan listas 'routine' y/o 'bonus' en delivery_types.\")\n",
    "        raise ValueError(\"delivery_types incompleto.\")\n",
    "\n",
    "    valid_rutina = [s.strip().upper() for s in config.delivery_types.routine or []]\n",
    "    valid_bonif  = [s.strip().upper() for s in config.delivery_types.bonus or []]\n",
    "\n",
    "    solapados = set(valid_rutina) & set(valid_bonif)\n",
    "    if solapados:\n",
    "        log_error(f\"delivery_types contiene códigos duplicados entre routine y bonus: {sorted(solapados)}\")\n",
    "        raise ValueError(f\"Códigos duplicados en delivery_types: {sorted(solapados)}\")\n",
    "\n",
    "    log_info(f\"delivery_types OK. rutina={valid_rutina}, bonif={valid_bonif}\")\n",
    "\n",
    "    # --- Validar unit_factors\n",
    "    uf_cfg = getattr(config, \"unit_factors\", None)\n",
    "    uf = OmegaConf.to_container(uf_cfg, resolve=True) if uf_cfg is not None else None\n",
    "    if not isinstance(uf, dict) or len(uf) == 0:\n",
    "        log_error(\"unit_factors debe ser un diccionario no vacío {unidad: factor}.\")\n",
    "        raise ValueError(\"unit_factors inválido o vacío.\")\n",
    "\n",
    "    norm_factors = {}\n",
    "    for k, v in uf.items():\n",
    "        key = str(k).strip().upper()\n",
    "        if not key:\n",
    "            log_error(\"unit_factors tiene una clave vacía.\")\n",
    "            raise ValueError(\"Clave vacía en unit_factors.\")\n",
    "        try:\n",
    "            val = float(v)\n",
    "        except Exception:\n",
    "            log_error(f\"unit_factors: valor no numérico para la clave '{k}': {v}\")\n",
    "            raise ValueError(f\"unit_factors inválido en '{k}'\")\n",
    "        if val <= 0:\n",
    "            log_error(f\"unit_factors: factor debe ser > 0 para '{key}', actual={val}\")\n",
    "            raise ValueError(f\"unit_factors con valor no válido en '{key}'\")\n",
    "        norm_factors[key] = val\n",
    "\n",
    "    keys = [F.lit(k) for k in norm_factors.keys()]\n",
    "    vals = [F.lit(v) for v in norm_factors.values()]\n",
    "    factor_map = F.map_from_arrays(F.array(*keys), F.array(*vals))\n",
    "    log_info(f\"unit_factors OK. {norm_factors}\")\n",
    "\n",
    "    # --- Finalización\n",
    "    log_info(\"Configuración validada correctamente.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_error(f\" Error durante la validación del config.yaml: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26541a6c-2862-4bd0-af5c-99015b72681f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ESQUEMAS USADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41604b96-bad4-4ee1-ba0f-ff0d4fc354ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS RDV;\n",
    "CREATE SCHEMA IF NOT EXISTS UDV;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf23f997-0a9e-42fd-8223-5c789b5472ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS RDV.data_ventas (\n",
    "  pais VARCHAR(2) ,\n",
    "  fecha_proceso DATE  ,\n",
    "  transporte STRING,\n",
    "  ruta STRING ,\n",
    "  tipo_entrega STRING,\n",
    "  material STRING,\n",
    "  precio DECIMAL(21,2) ,\n",
    "  cantidad DECIMAL(21,2) COMMENT 'Cantidad segun la unidad' ,\n",
    "  unidad VARCHAR(2) COMMENT 'Tipo de unidad (caja , unidad)'\n",
    ")\n",
    "USING DELTA\n",
    "PARTITIONED BY (fecha_proceso);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f733109e-2aa9-4edc-8e78-075e2f7d1018",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS UDV.data_ventas_depurado (\n",
    "  cod_pais VARCHAR(2) ,\n",
    "  fec_proceso DATE  ,\n",
    "  cod_transporte STRING,\n",
    "  cod_ruta STRING ,\n",
    "  cod_tipo_entrega STRING,\n",
    "  cod_material STRING,\n",
    "  precio_unitario_unidades DECIMAL(21,3) COMMENT 'Precio unitario por unidad' ,\n",
    "  mto_venta DECIMAL(21,2) ,\n",
    "  cant_uni_medida DECIMAL(21,2) COMMENT 'Cantidad segun la unidad de medida',\n",
    "  cod_uni_medida VARCHAR(2) COMMENT 'Tipo de unidad (caja , unidad)',\n",
    "  cant_unidades DECIMAL(21,2) COMMENT 'Cantidad en unidades',\n",
    "  ind_rutina BOOLEAN COMMENT 'Indicador de rutina',\n",
    "  ind_bonificacion BOOLEAN COMMENT 'Indicador de bonificación',\n",
    "  origen_datos STRING COMMENT 'Archivo origen',\n",
    "  fec_actualizacion_registro\tSTRING\tCOMMENT 'Fecha de actualización del registro (en formato string)'\n",
    ")\n",
    "USING DELTA\n",
    "PARTITIONED BY (fec_proceso)\n",
    "\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c092f74-d79b-40ff-b087-8de3208099b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS UDV.data_ventas_obs (\n",
    "  cod_pais STRING ,\n",
    "  fec_proceso STRING  ,\n",
    "  cod_transporte STRING,\n",
    "  cod_ruta STRING ,\n",
    "  cod_tipo_entrega STRING,\n",
    "  cod_material STRING,\n",
    "  mto_venta STRING ,\n",
    "  cant_uni_medida STRING,\n",
    "  cod_uni_medida STRING,\n",
    "  motivo_obs STRING COMMENT 'Motivo de la observación',\n",
    "  origen_datos STRING COMMENT 'Archivo origen',\n",
    "  fec_actualizacion_registro\tSTRING\tCOMMENT 'Fecha de actualización del registro (en formato string)'\n",
    ")\n",
    "USING DELTA\n",
    "PARTITIONED BY (fec_proceso);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd6dc237-a4fc-4a45-854a-12ab81bf2ff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "\n",
    "schema_csv = StructType([\n",
    "    StructField(\"pais\", StringType(), True),\n",
    "    StructField(\"fecha_proceso\", StringType(), True),\n",
    "    StructField(\"transporte\", StringType(), True),\n",
    "    StructField(\"ruta\", StringType(), True),\n",
    "    StructField(\"tipo_entrega\", StringType(), True),\n",
    "    StructField(\"material\", StringType(), True),\n",
    "    StructField(\"precio\", DoubleType(), True),\n",
    "    StructField(\"cantidad\", DoubleType(), True),\n",
    "    StructField(\"unidad\", StringType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6adfab26-f9e8-4ed9-a440-5b14e1428108",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### CAPA BRONZE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5193ce76-daf1-49c6-a747-c5ada76323c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "    \n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def procesar_pais_rdv(country, start_date, end_date):\n",
    "    try:\n",
    "\n",
    "        df_csv = (spark.read.schema(schema_csv)\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", False)\n",
    "                .csv(config.paths.raw_csv)) \n",
    "        \n",
    "        df_ventas = df_csv.select(\n",
    "        F.col(\"pais\"),\n",
    "        F.to_date(F.regexp_replace(F.col(\"fecha_proceso\"), r\"\\s+\", \"\"), \"yyyyMMdd\").alias(\"fecha_proceso\"),\n",
    "        F.col(\"transporte\"),\n",
    "        F.col(\"ruta\"),\n",
    "        F.col(\"tipo_entrega\"),\n",
    "        F.col(\"material\"),\n",
    "        F.col(\"precio\").cast(\"decimal(21,2)\"),\n",
    "        F.col(\"cantidad\").cast(\"decimal(21,2)\"),\n",
    "        F.col(\"unidad\")\n",
    "        )\n",
    "\n",
    "        # FILTRADO DE LOS DATOS SEGUN EL ARCHIVO YAML\n",
    "\n",
    "        df_ventas = df_ventas.filter(\n",
    "            (F.col(\"pais\") == country) &\n",
    "            (F.col(\"fecha_proceso\") >= start_date) &\n",
    "            (F.col(\"fecha_proceso\") <= end_date)\n",
    "        )\n",
    "\n",
    "        (df_ventas.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"replaceWhere\", f\"pais = '{country}' AND fecha_proceso >= DATE '{start_date}' AND fecha_proceso <= DATE '{end_date}'\")\n",
    "        .partitionBy(\"fecha_proceso\")\n",
    "        .saveAsTable(\"RDV.data_ventas\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        log_error(f\"{country}: Error durante el proceso  rdv {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b1f583a-c2fb-4525-9d8a-a50fec0d2865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### CAPA SILVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55a64545-ffb6-40c6-892a-fddfa9962b37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def actualizar_config(country):\n",
    "    cfg_copy = OmegaConf.to_container(config, resolve=True)\n",
    "    for p in cfg_copy[\"params\"]:\n",
    "        if p[\"country\"] == country:\n",
    "            p[\"proccess\"] = \"YES\"\n",
    "\n",
    "    OmegaConf.save(OmegaConf.create(cfg_copy), CFG_PATH)\n",
    "    print(f\"Config actualizado: {country} procesado\")\n",
    "\n",
    "factor_expr = F.element_at(\n",
    "    factor_map, \n",
    "    F.upper(F.trim(F.col(\"cod_uni_medida\")))\n",
    ")\n",
    "\n",
    "errores_concat = F.concat_ws(\n",
    "    \"|\",\n",
    "    F.when(F.col(\"cant_uni_medida\").isNull(), F.lit(\"ERR_CANT_NULL\")),\n",
    "    F.when(F.col(\"cant_uni_medida\").isNotNull() & (F.col(\"cant_uni_medida\") <= 0),\n",
    "           F.lit(\"ERR_CANT_NO_POSITIVA\")),\n",
    "    F.when(F.col(\"cod_material\").isNull(), F.lit(\"ERR_SIN_MATERIAL_CONOCIDO\")),\n",
    "    F.when(factor_expr.isNull(), F.lit(\"ERR_UNIDAD_DESCONOCIDA\")),\n",
    "    F.when(F.col(\"mto_venta\").isNotNull() & (F.col(\"mto_venta\") <= 0),\n",
    "           F.lit(\"ERR_VENTA_NO_POSITIVA\")),\n",
    "    F.when(~F.upper(F.trim(F.col(\"cod_tipo_entrega\"))).isin(valid_rutina + valid_bonif),\n",
    "           F.lit(\"ERR_TIPO_ENTREGA_NO_CONSIDERADA\"))\n",
    ")\n",
    "\n",
    "\n",
    "def procesar_pais_udv(country, start_date, end_date):\n",
    "    try:\n",
    "        df_rdv_ventas = spark.read.table(\"RDV.data_ventas\").filter(\n",
    "            (F.col(\"pais\") == country) &\n",
    "            (F.col(\"fecha_proceso\") >= start_date) &\n",
    "            (F.col(\"fecha_proceso\") <= end_date)\n",
    "        )\n",
    "\n",
    "        df_rdv_ventas = df_rdv_ventas.select(\n",
    "            F.col(\"pais\").alias(\"cod_pais\"),\n",
    "            F.col(\"fecha_proceso\").alias(\"fec_proceso\"),\n",
    "            F.col(\"transporte\").alias(\"cod_transporte\"),\n",
    "            F.col(\"ruta\").alias(\"cod_ruta\"),\n",
    "            F.col(\"tipo_entrega\").alias(\"cod_tipo_entrega\"),\n",
    "            F.col(\"material\").alias(\"cod_material\"),\n",
    "            F.col(\"precio\").alias(\"mto_venta\"),\n",
    "            F.col(\"cantidad\").alias(\"cant_uni_medida\"),\n",
    "            F.col(\"unidad\").alias(\"cod_uni_medida\")\n",
    "        )\n",
    "        \n",
    "        df_rdv_ventas = df_rdv_ventas.select(\n",
    "            F.col(\"cod_pais\"),\n",
    "            F.col(\"fec_proceso\"),\n",
    "            F.col(\"cod_transporte\"),\n",
    "            F.col(\"cod_ruta\"),\n",
    "            F.col(\"cod_tipo_entrega\"),\n",
    "            F.col(\"cod_material\"),\n",
    "            F.when(\n",
    "                factor_expr.isNotNull() &\n",
    "                F.col(\"cant_uni_medida\").isNotNull() & (F.col(\"cant_uni_medida\") > 0) &\n",
    "                F.col(\"mto_venta\").isNotNull(),\n",
    "                F.round(\n",
    "                    F.col(\"mto_venta\") / (F.col(\"cant_uni_medida\") * factor_expr),\n",
    "                    3\n",
    "                )\n",
    "            ).cast(\"decimal(21,3)\").alias(\"precio_unitario_unidades\"),\n",
    "            F.round(F.col(\"mto_venta\"), 2).cast(\"decimal(21,2)\").alias(\"mto_venta\"),\n",
    "            F.round(F.col(\"cant_uni_medida\"), 2).cast(\"decimal(21,2)\").alias(\"cant_uni_medida\"),\n",
    "            F.col(\"cod_uni_medida\"),\n",
    "            F.when(\n",
    "                factor_expr.isNotNull() &\n",
    "                F.col(\"cant_uni_medida\").isNotNull() & (F.col(\"cant_uni_medida\") > 0),\n",
    "                F.round(F.col(\"cant_uni_medida\") * factor_expr, 2)\n",
    "            ).cast(\"decimal(21,2)\").alias(\"cant_unidades\"),\n",
    "            F.when(F.upper(F.trim(F.col(\"cod_tipo_entrega\"))).isin(valid_rutina), True)\n",
    "            .when(F.upper(F.trim(F.col(\"cod_tipo_entrega\"))).isin(valid_bonif), False)\n",
    "            .otherwise(F.lit(None)).alias(\"ind_rutina\"),\n",
    "            F.when(F.upper(F.trim(F.col(\"cod_tipo_entrega\"))).isin(valid_bonif), True)\n",
    "            .when(F.upper(F.trim(F.col(\"cod_tipo_entrega\"))).isin(valid_rutina), False)\n",
    "            .otherwise(F.lit(None)).alias(\"ind_bonificacion\"),\n",
    "            F.lit(config.paths.raw_csv).alias(\"origen_datos\"),\n",
    "            F.date_format(F.current_timestamp(), \"yyyy-MM-dd HH:mm:ss\").alias(\"fec_actualizacion_registro\"),\n",
    "            F.when(F.length(errores_concat) == 0, F.lit(None)).otherwise(errores_concat).alias(\"motivo_obs\")\n",
    "        )\n",
    "\n",
    "        df_depurado = df_rdv_ventas.filter(\n",
    "            F.col(\"motivo_obs\").isNull()\n",
    "        ).drop(\"motivo_obs\")\n",
    "\n",
    "        (df_depurado.write\n",
    "        .format('delta')\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"replaceWhere\", f\"cod_pais = '{country}' AND fec_proceso >= DATE '{start_date}' AND fec_proceso <= DATE '{end_date}'\")\n",
    "        .partitionBy(\"fec_proceso\")\n",
    "        .saveAsTable(\"UDV.data_ventas_depurado\")\n",
    "        )\n",
    "\n",
    "        (df_depurado.write\n",
    "        .format('delta')\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"replaceWhere\", f\"cod_pais = '{country}' AND fec_proceso >= DATE '{start_date}' AND fec_proceso <= DATE '{end_date}'\")\n",
    "        .partitionBy(\"fec_proceso\")\n",
    "        .save(config.paths.output_root)\n",
    "        ) \n",
    "\n",
    "        df_obs = df_rdv_ventas.select(\n",
    "            F.col(\"cod_pais\").cast(\"string\"),\n",
    "            F.col(\"fec_proceso\").cast(\"string\"),\n",
    "            F.col(\"cod_transporte\").cast(\"string\"),\n",
    "            F.col(\"cod_ruta\").cast(\"string\"),\n",
    "            F.col(\"cod_tipo_entrega\").cast(\"string\"),\n",
    "            F.col(\"cod_material\").cast(\"string\"),\n",
    "            F.col(\"mto_venta\").cast(\"string\"),\n",
    "            F.col(\"cant_uni_medida\").cast(\"string\"),\n",
    "            F.col(\"cod_uni_medida\").cast(\"string\"),\n",
    "            F.col(\"motivo_obs\").cast(\"string\"),\n",
    "            F.col(\"origen_datos\").cast(\"string\"),\n",
    "            F.col(\"fec_actualizacion_registro\").cast(\"string\")\n",
    "        ).filter(\n",
    "            F.col(\"motivo_obs\").isNotNull()\n",
    "        )\n",
    "        \n",
    "        (df_obs.write.\n",
    "        format('delta')\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"replaceWhere\", f\"cod_pais = '{country}' AND  fec_proceso >=   '{start_date}' AND  fec_proceso <=   '{end_date}'\")\n",
    "        .partitionBy(\"fec_proceso\")\n",
    "        .saveAsTable(\"UDV.data_ventas_obs\")\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        log_error(f\"{country}: Error durante el proceso  UDV -> {str(e)}\")\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bd89a33-2e11-45df-8393-37599dfc844f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for param in config.params:\n",
    "    if param.proccess == \"YES\":\n",
    "        log_info(f\"Saltando país {param.country}: ya procesado previamente.\")\n",
    "        continue\n",
    "\n",
    "    if param.proccess == \"NO\":\n",
    "        country = param.country\n",
    "        start_date = param.start_date\n",
    "        end_date = param.end_date\n",
    "        \n",
    "        log_info(f\"Procesando país  {country} ({start_date} - {end_date})...\")\n",
    "        procesar_pais_rdv(country, start_date, end_date)\n",
    "        procesar_pais_udv(country, start_date, end_date)\n",
    "        actualizar_config(country)\n",
    "    \n",
    "cerrar_log()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7498097216776132,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4,
    "widgetLayout": []
   },
   "notebookName": "ETL_Entregas_Base",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
