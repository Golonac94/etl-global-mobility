#  ETL de Entregas de Producto ‚Äî Prueba T√©cnica (Grupo Mariposa)

##  Descripci√≥n General

Este proyecto implementa un **flujo ETL parametrizable en PySpark**, desarrollado como parte de una **prueba t√©cnica de ingenier√≠a de datos**.  
El objetivo es procesar y depurar registros de entregas de producto provenientes de distintos pa√≠ses, utilizando una arquitectura flexible controlada mediante **OmegaConf (YAML)** para definir par√°metros como fechas, pa√≠s y rutas de salida.

El flujo se dise√±√≥ para operar en entornos **develop / qa / main**, y genera salidas **particionadas por `fecha_proceso`** bajo una estructura estandarizada en formato **Delta Lake**.

---
 
## Objetivos

El pipeline cumple con los siguientes requerimientos definidos en la prueba t√©cnica:

1. Lectura de archivo CSV fuente.  
2. Filtrado por **rango de fechas din√°mico** (`start_date`, `end_date`) usando OmegaConf.  
3. Escritura de salidas **particionadas por `fecha_proceso`** en `data/processed/${fecha_proceso}`.  
4. Parametrizaci√≥n por pa√≠s (`country`).  
5. Estandarizaci√≥n de unidades (`CS` ‚Üí 20 `ST`).  
6. Identificaci√≥n de entregas:
   - `ZPRE`, `ZVE1` ‚Üí entregas de rutina.  
   - `Z04`, `Z05` ‚Üí entregas con bonificaci√≥n.  
7. Filtrado de registros no v√°lidos y generaci√≥n de tabla de observaciones.  
8. Estandarizaci√≥n de nombres de columnas bajo convenci√≥n `snake_case`.  
9. Detecci√≥n y eliminaci√≥n de anomal√≠as.  
10. Columnas adicionales fundamentadas:
    - `precio_unitario_unidades`
    - `ind_rutina`
    - `ind_bonificacion`

---

## ‚öôÔ∏è Arquitectura General del Flujo

<!-- TODO: Inserta aqu√≠ un diagrama tipo "data flow" mostrando las etapas:
CSV ‚Üí Bronze (lectura y limpieza b√°sica) ‚Üí Silver (transformaciones / unidad estandarizada / flags) ‚Üí Gold (salida particionada por fecha_proceso)
Usa draw.io o mermaid y guarda como `docs/etl_diagram.png` -->

**Etapas principales:**

1. **Lectura:**  
   Se lee el dataset CSV desde la ruta indicada en `config.yaml` (`paths.raw_csv`).

2. **Filtrado:**  
   Aplicaci√≥n din√°mica de filtros por pa√≠s y rango de fechas definidos en `params`.

3. **Transformaci√≥n:**  
   - Conversi√≥n de unidades (`CS ‚Üí ST * 20`).  
   - Generaci√≥n de columnas `ind_rutina`, `ind_bonificacion`.  
   - C√°lculo de `precio_unitario_unidades = mto_venta / cantidad_en_unidades`.  
   - Normalizaci√≥n de columnas (`snake_case`, trim, upper/lower seg√∫n tipo).

4. **Control de Calidad:**  
   - Se generan dos DataFrames:
     - `data_ventas_depurado`: registros v√°lidos.  
     - `data_ventas_obs`: registros descartados con motivo de observaci√≥n.  
   - Ambas tablas se guardan en formato **Delta**, particionadas por `fec_proceso`.

5. **Escritura:**  
   Los resultados se guardan en:
   ```
   data/processed/{fecha_proceso}/data_ventas_depurado
   data/processed/{fecha_proceso}/data_ventas_obs
   ```

---

## üßæ Configuraci√≥n (OmegaConf)

El archivo `config/config.yaml` define todos los par√°metros del flujo, la conversion unidades y los tipos de delivery validos  :

```yaml

paths:
  raw_csv: "/Volumes/workspace/global_mobility/data/raw/global_mobility_data_entrega_productos.csv"
  output_root: "/Volumes/workspace/global_mobility/data/processed"   

params:    #PARAMETROS EDICION
  start_date: "2025-01-01"        
  end_date:   "2025-06-30"
  country:    "PE"              

delivery_types:
    routine: ["ZPRE", "ZVE1"]
    bonus:   ["Z04", "Z05"]

unit_factors:
  "CS": 20
  "ST": 1

```

El flujo puede ejecutarse para cualquier rango de fechas y pa√≠s sin modificar el c√≥digo.

---

## Principales Transformaciones

| Columna origen | Regla aplicada | Columna resultante | Obsevaci√≥n |
|----------------|----------------|--------------------|--------------------|
| `unidad` | CS = 20 ST, ST = 1 | `cant_unidad_medida` | Cualquier otro valor da `null` y se guarda en obs |
| `mto_venta` / `cant_unidad_medida` | Precio unitario redondeado a 3 decimales | `precio_unitario_unidades` | Cantidad igual 0 dara `null` y se guarda en obs |
| `tipo_entrega` | Listado de config.yml | `ind_rutina`, `ind_bonificacion` |Cantidad igual 0 dara `null` y se guarda en obs |
| ‚Äî | Anomal√≠as detectadas | `motivo_obs` (en tabla observaciones) |

---

## üß™ Validaciones y Observaciones

- Se crean **dos salidas**:
  - `data_ventas_depurado`: registros v√°lidos.
  - `data_ventas_obs`: registros descartados con detalle del motivo.

- Ejemplo de escritura:
  ```python
  df_clean.write.format("delta").mode("overwrite").partitionBy("fec_proceso").save(out_path)
  df_obs.write.format("delta").mode("overwrite").partitionBy("fec_proceso").save(obs_path)
  ```

<!-- TODO: agrega una captura de pantalla del resultado en Databricks mostrando las particiones o vista Delta -->

---

## üß† Est√°ndar de Columnas Finales

| Campo | Tipo | Descripci√≥n |
|--------|------|-------------|
| `cod_pais` | STRING | C√≥digo ISO del pa√≠s |
| `fec_proceso` | DATE | Fecha de procesamiento (partici√≥n) |
| `cod_transporte` | STRING | Identificador del transporte |
| `cod_ruta` | STRING | C√≥digo de ruta |
| `cod_tipo_entrega` | STRING | Tipo de entrega (ZPRE, Z04, etc.) |
| `cod_material` | STRING | Material entregado |
| `mto_venta` | DECIMAL | Monto de venta |
| `cant_unidad_medida` | DECIMAL | Cantidad estandarizada (ST) |
| `cod_unidad_medida` | STRING | Unidad estandarizada (ST) |
| `precio_unitario_unidades` | DECIMAL(21,3) | Precio por unidad est√°ndar |
| `ind_rutina` | INT | 1 = rutina, 0 = no |
| `ind_bonificacion` | INT | 1 = bonificaci√≥n, 0 = no |
| `origen_datos` | STRING | Archivo o fuente original |
| `fec_actualizacion_registro` | STRING | Fecha de √∫ltima actualizaci√≥n |

---
